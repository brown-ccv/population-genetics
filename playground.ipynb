{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "standardize data...\n",
      "done\n",
      "calculate Fisher discriminant ratio (FDR)...\n",
      "done\n",
      "rank the features...\n",
      "   features sorted in order of how well they discriminate between different classes (first item is best):\n",
      "   ['ThetaPi_1' 'H2.H1_1' 'H1_1' 'Theta1Pi_1' 'H12_1' 'ThetaS_1' 'DAF_1'\n",
      " 'Theta1S_1' 'TajD_1' 'FuLiF_1' 'Theta1L_1' 'FuLiD_1' 'DXPEHH_12'\n",
      " 'FuLiF1_1' 'TajD1_1' 'DXPEHH_13\\n' 'FuLiD1_1' 'FayWuH_1' 'FST_1'\n",
      " 'XPEHH_12' 'SL1_1' 'H2_1' 'XPEHH_13' 'ThetaL_1' 'SL0_1' 'iHH0_1'\n",
      " 'Theta1H_1' 'DDAF_1' 'Theta1Xi_1' 'nSL_1' 'iHS_1' 'FayWuH1_1' 'DnSL_1'\n",
      " 'iHH1_1' 'ZengE1_1' 'MAF_1' 'ZengE_1' 'ZA_1' 'ThetaXi_1' 'DiHH_1'\n",
      " 'ThetaH_1']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "standardize = True\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "            header = np.array(header)\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "if standardize:\n",
    "    print 'standardize data...'\n",
    "    # This is necessary because different features have different dynamic ranges. Standardization brings all features \n",
    "    # to the same scale.\n",
    "\n",
    "    mean = np.mean(data,axis=0)\n",
    "    std = np.std(data,axis=0)\n",
    "\n",
    "    data_standard = np.zeros(np.shape(data))\n",
    "    data_mask = np.zeros(np.shape(data))\n",
    "    # save the classes\n",
    "    data_standard[:,0] = data[:,0]\n",
    "    data_mask = np.isfinite(data)\n",
    "    # standardize data\n",
    "    for i in range(41):\n",
    "        a = data[:,i+1]\n",
    "        mask_a = data_mask[:,i+1]\n",
    "        mean = np.mean(a[mask_a]) \n",
    "        std = np.std(a[mask_a])\n",
    "\n",
    "        data_standard[mask_a,i+1] = (a[mask_a] - mean)/std\n",
    "\n",
    "    print 'done'\n",
    "\n",
    "    print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "    # FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "    # sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "    # The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "    # are better at distinguishing classes.\n",
    "    # For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "\n",
    "    FDR = np.zeros(41)\n",
    "    for i in range(41):\n",
    "        FDR_sum = 0e0\n",
    "        for j in range(5):\n",
    "            mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "            for k in range(j):\n",
    "                mask_k = data_mask[:,i+1] & (data_standard[:,0] == k)\n",
    "\n",
    "                mu_j = np.mean(data_standard[mask_j,i+1])\n",
    "                mu_k = np.mean(data_standard[mask_k,i+1])\n",
    "                sigma_j = np.var(data_standard[mask_j,i+1])\n",
    "                sigma_k = np.var(data_standard[mask_k,i+1])\n",
    "\n",
    "                FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "        # check the values     \n",
    "        #print i,FDR_sum\n",
    "        #for j in range(5):\n",
    "        #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "        FDR[i] = FDR_sum\n",
    "else:\n",
    "    print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "    # FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "    # sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "    # The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "    # are better at distinguishing classes.\n",
    "    # For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "    \n",
    "    data_mask = np.zeros(np.shape(data))\n",
    "    data_mask = np.isfinite(data)\n",
    "    \n",
    "    FDR = np.zeros(41)\n",
    "    for i in range(41):\n",
    "        FDR_sum = 0e0\n",
    "        for j in range(5):\n",
    "            mask_j = data_mask[:,i+1] & (data[:,0] == j)\n",
    "            for k in range(j):\n",
    "                mask_k = data_mask[:,i+1] & (data[:,0] == k)\n",
    "\n",
    "                mu_j = np.mean(data[mask_j,i+1])\n",
    "                mu_k = np.mean(data[mask_k,i+1])\n",
    "                sigma_j = np.var(data[mask_j,i+1])\n",
    "                sigma_k = np.var(data[mask_k,i+1])\n",
    "\n",
    "                FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "        # check the values     \n",
    "        #print i,FDR_sum\n",
    "        #for j in range(5):\n",
    "        #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "        FDR[i] = FDR_sum\n",
    "    \n",
    "    \n",
    "print 'done'\n",
    "\n",
    "print 'rank the features...'\n",
    "indx_sorted = np.argsort(FDR)[::-1]\n",
    "print '   features sorted in order of how well they discriminate between different classes (first item is best):'\n",
    "print '  ',header[indx_sorted]\n",
    "\n",
    "# add 1 and insert 0 to the first place to keep the class in.\n",
    "indx_sorted = np.insert(indx_sorted+1,0,0)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.177827941004\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.316227766017\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.56234132519\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "1.0\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:40: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:41: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:43: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:44: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# general naive bayes classifier\n",
    "# the true probability distribution of the features is estimated using a gaussian kernel density estimator\n",
    "# https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "# scipy's kde is fastest for a few 100 data points.\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "n_sim = 100\n",
    "\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "# test what bandwidth value gives best scores\n",
    "bandwidth2 = 10e0**(np.linspace(-1,0,num=5,endpoint = True))\n",
    "\n",
    "\n",
    "for bw in bandwidth2:\n",
    "    print bw\n",
    "    \n",
    "    # run n_sim different simulations to get a feeling of random effects\n",
    "    for ii in range(n_sim):\n",
    "        if ii%10 == 0:\n",
    "            print '   ',ii\n",
    "\n",
    "        # remove one feature at a time (the least disciminative one)\n",
    "        for jj in range(40):\n",
    "\n",
    "            if standardize:\n",
    "                data_to_use = data_standard[:,indx_sorted[:-(jj+1)]]\n",
    "            else:\n",
    "                data_to_use = data[:,indx_sorted[:-(jj+1)]] \n",
    "\n",
    "            nr_classes = len(np.unique(data_to_use[:,0]))\n",
    "            nr_features = np.shape(data_to_use[:,1:])[1]\n",
    "\n",
    "            # shuffle and divide up the data\n",
    "            n = np.shape(data_to_use)[0]\n",
    "            indx = np.arange(n)\n",
    "            np.random.shuffle(indx)\n",
    "\n",
    "            X_train = data_to_use[indx[:n*0.75],1:]\n",
    "            Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "            X_test = data_to_use[indx[n*0.75:],1:]\n",
    "            Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "\n",
    "            # collect kernels for each class and feature\n",
    "            kernels = []\n",
    "            for i in range(nr_classes):\n",
    "                mask = Y_train == i\n",
    "                points_in_class = X_train[mask,:]\n",
    "\n",
    "                kernels_class_i = []\n",
    "                \n",
    "                for j in range(nr_features):\n",
    "                    feature_j = points_in_class[:,j]\n",
    "                    mask = np.isfinite(feature_j)\n",
    "\n",
    "                    kernels_class_i.append(gaussian_kde(feature_j[mask],bw_method = bw))\n",
    "\n",
    "                kernels.append(kernels_class_i)\n",
    "\n",
    "\n",
    "            # loop through the test points and estimate the most likely class\n",
    "            score = 0e0\n",
    "            for i in range(len(Y_test)): \n",
    "\n",
    "                class_prob = np.zeros(nr_classes)\n",
    "                for j in range(nr_classes):\n",
    "                    kernels_class_j = kernels[j]\n",
    "                    \n",
    "                    #for k in range(nr_features):\n",
    "                    #    kernel = kernels_class_j[k]\n",
    "                    #    class_prob[j] = class_prob[j] + kernel.logpdf(X_test[i,k])\n",
    "                    \n",
    "                    class_prob[j] = np.sum([kernel.logpdf(point) for kernel, point in zip(kernels_class_j, X_test[i,:])])\n",
    "\n",
    "                if np.argmax(class_prob) == Y_test[i]:\n",
    "                    score = score + 1\n",
    "\n",
    "            test_score[ii,jj] = score / len(Y_test)\n",
    "\n",
    "    # make a plot            \n",
    "    plt.close()\n",
    "    plt.ylim([0,80])\n",
    "    plt.errorbar(41 - (np.arange(40)+1),np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "    plt.xlabel('nr. of features used')\n",
    "    plt.ylabel('test score [%]')\n",
    "    if standardize:\n",
    "        plt.savefig('general_naive_bayes_standardized_bw'+str(bw)+'_'+str(n_sim)+'.png')\n",
    "    else:\n",
    "        plt.savefig('general_naive_bayes_bw'+str(bw)+'_'+str(n_sim)+'.png')\n",
    "    plt.close()\n",
    "    print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
