{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation coefficient matrix and the cloud of points for every pair of variables. This helps to familiarize myself with the data, it allows me to check how strongly correlated some variables are and what type of correlation is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "prepare the plots...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "print 'prepare the plots...'\n",
    "\n",
    "# correlation coefficient matrix, masking out nans (didn't find a way to do this without for loops)\n",
    "corr_coef = np.zeros([41,41])\n",
    "for i in range(41):\n",
    "    for j in range(41):\n",
    "        a = data[:,i+1]\n",
    "        b = data[:,j+1]\n",
    "        mask_a = np.isfinite(a)\n",
    "        mask_b = np.isfinite(b)\n",
    "        mask = mask_a & mask_b\n",
    "        corr_coef[i,j] = np.corrcoef([a[mask],b[mask]])[0,1]\n",
    "\n",
    "plt.title('white - no corr., red - positive corr., blue - negative corr.')\n",
    "plt.imshow(corr_coef,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "plt.savefig('corr_coef.png')\n",
    "plt.close()\n",
    "\n",
    "# covariance matrix, masking out nans (didn't find a way to do this without for loops)\n",
    "cov = np.zeros([41,41])\n",
    "for i in range(41):\n",
    "    for j in range(41):\n",
    "        a = data[:,i+1]\n",
    "        b = data[:,j+1]\n",
    "        mask_a = np.isfinite(a)\n",
    "        mask_b = np.isfinite(b)\n",
    "        mask = mask_a & mask_b\n",
    "        cov[i,j] = np.cov([a[mask],b[mask]])[0,1]\n",
    "\n",
    "plt.imshow(cov,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "plt.savefig('covariance.png')\n",
    "plt.close()\n",
    "\n",
    "for i in range(41):\n",
    "    for j in range(i):\n",
    "        # i+1 and j+1 to skip the first row of classes\n",
    "        for k in range(5):\n",
    "            mask = (data[:,0] == k)\n",
    "            plt.plot(data[mask,i+1],data[mask,j+1],'+')\n",
    "        plt.xlabel(header[i])\n",
    "        plt.ylabel(header[j])\n",
    "        plt.savefig('imgs/'+header[i]+'-'+header[j]+'.png')\n",
    "        plt.close()\n",
    "        \n",
    "print 'done'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Class separability or class discriminatory power of the features.\n",
    "Calculate the Fisher's discriminant ratio for each feature and rank the features in descending order. The first feature in the list has the highest class separability of all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "standardize data...\n",
      "done\n",
      "calculate Fisher discriminant ratio (FDR)...\n",
      "done\n",
      "rank the features...\n",
      "   features sorted in order of how well they discriminate between different classes (first item is best):\n",
      "   ['ThetaPi_1' 'H2.H1_1' 'H1_1' 'Theta1Pi_1' 'H12_1' 'ThetaS_1' 'DAF_1'\n",
      " 'Theta1S_1' 'TajD_1' 'FuLiF_1' 'Theta1L_1' 'FuLiD_1' 'DXPEHH_12'\n",
      " 'FuLiF1_1' 'TajD1_1' 'DXPEHH_13\\n' 'FuLiD1_1' 'FayWuH_1' 'FST_1'\n",
      " 'XPEHH_12' 'SL1_1' 'H2_1' 'XPEHH_13' 'ThetaL_1' 'SL0_1' 'iHH0_1'\n",
      " 'Theta1H_1' 'DDAF_1' 'Theta1Xi_1' 'nSL_1' 'iHS_1' 'FayWuH1_1' 'DnSL_1'\n",
      " 'iHH1_1' 'ZengE1_1' 'MAF_1' 'ZengE_1' 'ZA_1' 'ThetaXi_1' 'DiHH_1'\n",
      " 'ThetaH_1']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "standardize = True\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "            header = np.array(header)\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "if standardize:\n",
    "    print 'standardize data...'\n",
    "    # This is necessary because different features have different dynamic ranges. Standardization brings all features \n",
    "    # to the same scale.\n",
    "\n",
    "    mean = np.mean(data,axis=0)\n",
    "    std = np.std(data,axis=0)\n",
    "\n",
    "    data_standard = np.zeros(np.shape(data))\n",
    "    data_mask = np.zeros(np.shape(data))\n",
    "    # save the classes\n",
    "    data_standard[:,0] = data[:,0]\n",
    "    data_mask = np.isfinite(data)\n",
    "    # standardize data\n",
    "    for i in range(41):\n",
    "        a = data[:,i+1]\n",
    "        mask_a = data_mask[:,i+1]\n",
    "        mean = np.mean(a[mask_a]) \n",
    "        std = np.std(a[mask_a])\n",
    "\n",
    "        data_standard[mask_a,i+1] = (a[mask_a] - mean)/std\n",
    "\n",
    "    print 'done'\n",
    "\n",
    "    print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "    # FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "    # sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "    # The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "    # are better at distinguishing classes.\n",
    "    # For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "\n",
    "    FDR = np.zeros(41)\n",
    "    for i in range(41):\n",
    "        FDR_sum = 0e0\n",
    "        for j in range(5):\n",
    "            mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "            for k in range(j):\n",
    "                mask_k = data_mask[:,i+1] & (data_standard[:,0] == k)\n",
    "\n",
    "                mu_j = np.mean(data_standard[mask_j,i+1])\n",
    "                mu_k = np.mean(data_standard[mask_k,i+1])\n",
    "                sigma_j = np.var(data_standard[mask_j,i+1])\n",
    "                sigma_k = np.var(data_standard[mask_k,i+1])\n",
    "\n",
    "                FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "        # check the values     \n",
    "        #print i,FDR_sum\n",
    "        #for j in range(5):\n",
    "        #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "        FDR[i] = FDR_sum\n",
    "else:\n",
    "    print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "    # FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "    # sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "    # The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "    # are better at distinguishing classes.\n",
    "    # For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "    \n",
    "    data_mask = np.zeros(np.shape(data))\n",
    "    data_mask = np.isfinite(data)\n",
    "    \n",
    "    FDR = np.zeros(41)\n",
    "    for i in range(41):\n",
    "        FDR_sum = 0e0\n",
    "        for j in range(5):\n",
    "            mask_j = data_mask[:,i+1] & (data[:,0] == j)\n",
    "            for k in range(j):\n",
    "                mask_k = data_mask[:,i+1] & (data[:,0] == k)\n",
    "\n",
    "                mu_j = np.mean(data[mask_j,i+1])\n",
    "                mu_k = np.mean(data[mask_k,i+1])\n",
    "                sigma_j = np.var(data[mask_j,i+1])\n",
    "                sigma_k = np.var(data[mask_k,i+1])\n",
    "\n",
    "                FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "        # check the values     \n",
    "        #print i,FDR_sum\n",
    "        #for j in range(5):\n",
    "        #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "        FDR[i] = FDR_sum\n",
    "    \n",
    "    \n",
    "print 'done'\n",
    "\n",
    "print 'rank the features...'\n",
    "indx_sorted = np.argsort(FDR)[::-1]\n",
    "print '   features sorted in order of how well they discriminate between different classes (first item is best):'\n",
    "print '  ',header[indx_sorted]\n",
    "\n",
    "# add 1 and insert 0 to the first place to keep the class in.\n",
    "indx_sorted = np.insert(indx_sorted+1,0,0)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use an SVM to do classification\n",
    "separate the data into training, cross validation, and test data sets (60-20-20%).\n",
    "make a loop through successively less features:\n",
    "    - use all features to find the best values for C and gamma in the SVM\n",
    "    - fix the best C and gamma values and successively remove a feature that is least discriminative\n",
    "    - check what number of features give the best score in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove one feature at a time (the least discriminative one) and calculate the classification score...\n",
      "    0\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:28: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:31: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:32: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# run the previous cell first!\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# do everything 100 times to characterize the noise\n",
    "\n",
    "print 'Train the SVM and find the best C and gamma parameters...'\n",
    "C_collect = []\n",
    "gamma_collect = []\n",
    "for i in range(100):\n",
    "    print '   round',i\n",
    "\n",
    "    # I can only use those data points that do not contain NAs\n",
    "    mask_to_use = np.all(np.isfinite(data),axis=1)\n",
    "    \n",
    "    if standardize:\n",
    "        data_to_use = data_standard[mask_to_use,:]\n",
    "    else:\n",
    "        data_to_use = data[mask_to_use,:]\n",
    "    \n",
    "    # nr of data points left\n",
    "    n = np.shape(data_to_use)[0]\n",
    "    \n",
    "    # shuffle and divide up the data\n",
    "    indx = np.arange(n)\n",
    "    np.random.shuffle(indx)\n",
    "    \n",
    "    X_train = data_to_use[indx[:n*0.6],1:]\n",
    "    Y_train = data_to_use[indx[:n*0.6],0]\n",
    "    \n",
    "    X_test = data_to_use[indx[n*0.6:n*0.8],1:]\n",
    "    Y_test = data_to_use[indx[n*0.6:n*0.8],0]\n",
    "    \n",
    "    X_CV = data_to_use[indx[n*0.8:],1:]\n",
    "    Y_CV = data_to_use[indx[n*0.8:],0]\n",
    "    \n",
    "    # parameter ranges for the SVM\n",
    "    C = 10e0**(np.linspace(-8e0,3e0,12))\n",
    "    gamma = 10e0**(np.linspace(-5e0,6e0,12))\n",
    "    \n",
    "    # arrays to store scores and best parameters\n",
    "    train_score = np.zeros([len(C),len(gamma)])\n",
    "    test_score = np.zeros([len(C),len(gamma)])\n",
    "    C_array = np.zeros([len(C),len(gamma)])\n",
    "    gamma_array = np.zeros([len(C),len(gamma)])\n",
    "    # loop through the C and gamma arrays\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(gamma)):\n",
    "            SVM = SVC(kernel='rbf', C=C[i], gamma=gamma[j]).fit(X_train, Y_train)\n",
    "            train_score[i,j] = SVM.score(X_train,Y_train)\n",
    "            test_score[i,j] = SVM.score(X_test,Y_test)\n",
    "            C_array[i,j] = C[i]\n",
    "            gamma_array[i,j] = gamma[j]\n",
    "\n",
    "    # find the C and gamma parameters that give max score.\n",
    "    # if there are multiple parameter configuration giving max score, the first one of these is used below \n",
    "    best_params = np.where(test_score == np.max(test_score))\n",
    "    print '      best C value(s):',C[best_params[0]]\n",
    "    print '      best gamma value(s):',gamma[best_params[1]]\n",
    "    print '      max test score:',np.max(test_score)\n",
    "    # calculate the cross validation score\n",
    "    for j in range(len(best_params[0])):\n",
    "        print '      the cross validation score for C='+str(C[best_params[0][j]])+' and gamma='+str(gamma[best_params[1][j]])+':'\n",
    "        SVM = SVC(kernel='rbf', C=C[best_params[0][j]], gamma=gamma[best_params[1][j]]).fit(X_train, Y_train)\n",
    "        CV_score = SVM.score(X_CV,Y_CV)\n",
    "        print '         ',CV_score\n",
    "    \n",
    "    C_collect.append(C[best_params[0]])\n",
    "    gamma_collect.append(gamma[best_params[1]])\n",
    "\n",
    "print 'done'\n",
    "\n",
    "# flatten the *_collect lists and find the most frequently occuring elements\n",
    "C_flat = [item for sublist in C_collect for item in sublist]\n",
    "gamma_flat = [item for sublist in gamma_collect for item in sublist]\n",
    "\n",
    "combined = [t for t in zip(C_flat,gamma_flat)]\n",
    "(C, gamma) = max(set(combined), key=combined.count)\n",
    "\n",
    "print 'The best C and gamma values are',C,'and',gamma,', respectively.'\n",
    "\n",
    "print 'Remove one feature at a time (the least discriminative one) and calculate the classification score...'\n",
    "\n",
    "n_sim = 10\n",
    "train_score = np.zeros([n_sim,40])\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "for i in range(n_sim):\n",
    "    if i%10 == 0:\n",
    "        print '   ',i\n",
    "    for j in range(40):        \n",
    "        # I can only use those data points that do not contain NAs\n",
    "        mask_to_use = np.all(np.isfinite(data[:,indx_sorted[:-(j+1)]]),axis=1)\n",
    "        \n",
    "        if standardize:\n",
    "            data_to_use = data_standard[:,indx_sorted[:-(j+1)]]  \n",
    "        else:\n",
    "            data_to_use = data[:,indx_sorted[:-(j+1)]]\n",
    "        \n",
    "        data_to_use = data_to_use[mask_to_use,:]\n",
    "                        \n",
    "        # nr of data points left\n",
    "        n = np.shape(data_to_use)[0]\n",
    "                \n",
    "        # shuffle and divide up the data\n",
    "        indx = np.arange(n)\n",
    "        np.random.shuffle(indx)\n",
    "\n",
    "        X_train = data_to_use[indx[:n*0.75],1:]\n",
    "        Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "        X_test = data_to_use[indx[n*0.75:],1:]\n",
    "        Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "        SVM = SVC(kernel='rbf', C=C, gamma=gamma).fit(X_train, Y_train)\n",
    "        train_score[i,j] = SVM.score(X_train,Y_train)\n",
    "        test_score[i,j] = SVM.score(X_test,Y_test)\n",
    "                \n",
    "\n",
    "# make a plot            \n",
    "plt.close()\n",
    "plt.ylim([0,80])\n",
    "plt.errorbar(41 - (np.arange(40)+1),np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "plt.xlabel('nr. of features used')\n",
    "plt.ylabel('test score [%]')\n",
    "if standardize:\n",
    "    plt.savefig('classification_standardized_'+str(n_sim)+'.png')\n",
    "else:\n",
    "    plt.savefig('classification_'+str(n_sim)+'.png')\n",
    "plt.close()\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gaussian naive bayes classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:19: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:20: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:25: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:26: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [  1 900]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-124d6c9917d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[1;32m    175\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [  1 900]"
     ]
    }
   ],
   "source": [
    "# trying a naive bayes classifier of sklearn\n",
    "# it does not handle missing data!\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if standardize:\n",
    "    data_to_use = data_standard\n",
    "else:\n",
    "    data_to_use = data\n",
    "\n",
    "data_mask = np.zeros(np.shape(data))\n",
    "data_mask = np.isfinite(data)\n",
    "    \n",
    "# shuffle and divide up the data\n",
    "n = np.shape(data_to_use)[0]\n",
    "indx = np.arange(n)\n",
    "np.random.shuffle(indx)\n",
    "\n",
    "X_train = data_to_use[indx[:n*0.6],1:]\n",
    "Y_train = data_to_use[indx[:n*0.6],0]\n",
    "\n",
    "X_test = data_to_use[indx[n*0.6:n*0.8],1:]\n",
    "Y_test = data_to_use[indx[n*0.6:n*0.8],0]\n",
    "\n",
    "X_CV = data_to_use[indx[n*0.8:],1:]\n",
    "Y_CV = data_to_use[indx[n*0.8:],0]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train[np.isfinite(X_train)], Y_train).predict(X_test[np.isfinite(X_test)])\n",
    "\n",
    "print (Y_test != y_pred).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:40: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:41: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:43: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:44: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# gaussian naive bayes classifier from scratch that handles missing data\n",
    "# not optimized!\n",
    "# run the FDR ranking cell first!\n",
    "\n",
    "n_sim = 300\n",
    "\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "# run n_sim different simulations to get a feeling of random effects\n",
    "for ii in range(n_sim):\n",
    "\n",
    "    if ii%10 == 0:\n",
    "        print ii\n",
    "    \n",
    "    # remove one feature at a time (the least disciminative one)\n",
    "    for jj in range(40):\n",
    "\n",
    "        if standardize:\n",
    "            data_to_use = data_standard[:,indx_sorted[:-(jj+1)]]\n",
    "        else:\n",
    "            data_to_use = data[:,indx_sorted[:-(jj+1)]] \n",
    "\n",
    "        \n",
    "        # arrays used to summarize data\n",
    "        # size of arrays: [nr of classes, nr of features]\n",
    "        nr_classes = len(np.unique(data_to_use[:,0]))\n",
    "        nr_features = np.shape(data_to_use[:,1:])[1]\n",
    "\n",
    "        mean = np.zeros([nr_classes,nr_features])\n",
    "        std = np.zeros(np.shape(mean))\n",
    "        # correction factor for the probabilities: its value is the fraction of data points used to calculate the mean and std\n",
    "        #   1 if all points are used, 0.1 if only 10% of points are used\n",
    "        corr_factor = np.zeros(np.shape(mean))\n",
    "\n",
    "\n",
    "        # shuffle and divide up the data\n",
    "        n = np.shape(data_to_use)[0]\n",
    "        indx = np.arange(n)\n",
    "        np.random.shuffle(indx)\n",
    "\n",
    "        X_train = data_to_use[indx[:n*0.75],1:]\n",
    "        Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "        X_test = data_to_use[indx[n*0.75:],1:]\n",
    "        Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "\n",
    "        # fill up the mean, std, and corr_factor arrays\n",
    "        for i in range(nr_classes):\n",
    "            mask = Y_train == i\n",
    "            points_in_class = X_train[mask,:]\n",
    "\n",
    "            for j in range(nr_features):\n",
    "                feature_j = points_in_class[:,j]\n",
    "                mask = np.isfinite(feature_j)\n",
    "\n",
    "                mean[i,j] = np.mean(feature_j[mask])\n",
    "                std[i,j] = np.std(feature_j[mask])\n",
    "                corr_factor[i,j] = float(np.sum(mask)) / float(len(feature_j))\n",
    "\n",
    "\n",
    "        # loop through the test points and estimate the most likely class\n",
    "        score = 0e0\n",
    "        for i in range(len(Y_test)): \n",
    "            \n",
    "            class_prob = np.zeros(nr_classes)\n",
    "            for j in range(nr_classes):\n",
    "                \n",
    "                # calculate the log probabilities that this data point was sampled from class j\n",
    "                # adding log probabilities is better than multiplying probabilities \n",
    "                log_factor = np.log(corr_factor[j,:] / (np.sqrt(2e0*np.pi)*std[j,:]))\n",
    "                log_exponent = -((X_test[i,:]-mean[j,:])**2e0/(2e0*std[j,:]**2e0))\n",
    "                log_prob = log_factor + log_exponent\n",
    "                class_prob[j] = np.sum(log_prob[np.isfinite(log_prob)])\n",
    "\n",
    "            if np.argmax(class_prob) == Y_test[i]:\n",
    "                score = score + 1\n",
    "\n",
    "        test_score[ii,jj] = score / len(Y_test)\n",
    "\n",
    "\n",
    "# make a plot            \n",
    "plt.close()\n",
    "plt.ylim([0,80])\n",
    "plt.errorbar(41 - (np.arange(40)+1),np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "plt.xlabel('nr. of features used')\n",
    "plt.ylabel('test score [%]')\n",
    "if standardize:\n",
    "    plt.savefig('naive_bayes_standardized_'+str(n_sim)+'.png')\n",
    "else:\n",
    "    plt.savefig('naive_bayes_'+str(n_sim)+'.png')\n",
    "plt.close()\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.0158489319246\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.0251188643151\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.0398107170553\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.063095734448\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n",
      "0.1\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:39: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:40: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:43: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# general naive bayes classifier\n",
    "# the true probability distribution of the features is estimated using a gaussian kernel density estimator\n",
    "# https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "# scipy's kde is fastest for a few 100 data points.\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "n_sim = 100\n",
    "\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "# test what bandwidth value gives best scores\n",
    "bandwidth = 10e0**(np.linspace(-2,-1,num=6,endpoint = True))\n",
    "\n",
    "for bw in bandwidth:\n",
    "    print bw\n",
    "    \n",
    "    # run n_sim different simulations to get a feeling of random effects\n",
    "    for ii in range(n_sim):\n",
    "        if ii%10 == 0:\n",
    "            print '   ',ii\n",
    "\n",
    "        # remove one feature at a time (the least disciminative one)\n",
    "        for jj in range(40):\n",
    "\n",
    "            if standardize:\n",
    "                data_to_use = data_standard[:,indx_sorted[:-(jj+1)]]\n",
    "            else:\n",
    "                data_to_use = data[:,indx_sorted[:-(jj+1)]] \n",
    "\n",
    "            nr_classes = len(np.unique(data_to_use[:,0]))\n",
    "            nr_features = np.shape(data_to_use[:,1:])[1]\n",
    "\n",
    "            # shuffle and divide up the data\n",
    "            n = np.shape(data_to_use)[0]\n",
    "            indx = np.arange(n)\n",
    "            np.random.shuffle(indx)\n",
    "\n",
    "            X_train = data_to_use[indx[:n*0.75],1:]\n",
    "            Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "            X_test = data_to_use[indx[n*0.75:],1:]\n",
    "            Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "\n",
    "            # collect kernels for each class and feature\n",
    "            kernels = []\n",
    "            for i in range(nr_classes):\n",
    "                mask = Y_train == i\n",
    "                points_in_class = X_train[mask,:]\n",
    "\n",
    "                kernels_class_i = []\n",
    "                \n",
    "                for j in range(nr_features):\n",
    "                    feature_j = points_in_class[:,j]\n",
    "                    mask = np.isfinite(feature_j)\n",
    "\n",
    "                    kernels_class_i.append(gaussian_kde(feature_j[mask],bw_method = bw))\n",
    "\n",
    "                kernels.append(kernels_class_i)\n",
    "\n",
    "\n",
    "            # loop through the test points and estimate the most likely class\n",
    "            score = 0e0\n",
    "            for i in range(len(Y_test)): \n",
    "\n",
    "                class_prob = np.zeros(nr_classes)\n",
    "                for j in range(nr_classes):\n",
    "                    kernels_class_j = kernels[j]\n",
    "                    \n",
    "                    #for k in range(nr_features):\n",
    "                    #    kernel = kernels_class_j[k]\n",
    "                    #    class_prob[j] = class_prob[j] + kernel.logpdf(X_test[i,k])\n",
    "                        \n",
    "                    class_prob[j] = np.sum([kernel.logpdf(point) for kernel, point in zip(kernels_class_j, X_test[i,:])])\n",
    "\n",
    "                if np.argmax(class_prob) == Y_test[i]:\n",
    "                    score = score + 1\n",
    "\n",
    "            test_score[ii,jj] = score / len(Y_test)\n",
    "\n",
    "    # make a plot            \n",
    "    plt.close()\n",
    "    plt.ylim([0,80])\n",
    "    plt.errorbar(41 - (np.arange(40)+1),np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "    plt.xlabel('nr. of features used')\n",
    "    plt.ylabel('test score [%]')\n",
    "    if standardize:\n",
    "        plt.savefig('general_naive_bayes_standardized_bw'+str(bw)+'_'+str(n_sim)+'.png')\n",
    "    else:\n",
    "        plt.savefig('general_naive_bayes_bw'+str(bw)+'_'+str(n_sim)+'.png')\n",
    "    plt.close()\n",
    "    print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
