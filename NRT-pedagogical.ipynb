{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation coefficient matrix and the cloud of points for every pair of variables. This helps to familiarize myself with the data, it allows me to check how strongly correlated some variables are and what type of correlation is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "prepare the plots...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "print 'prepare the plots...'\n",
    "\n",
    "# correlation coefficient matrix, masking out nans (didn't find a way to do this without for loops)\n",
    "corr_coef = np.zeros([41,41])\n",
    "for i in range(41):\n",
    "    for j in range(41):\n",
    "        a = data[:,i+1]\n",
    "        b = data[:,j+1]\n",
    "        mask_a = np.isfinite(a)\n",
    "        mask_b = np.isfinite(b)\n",
    "        mask = mask_a & mask_b\n",
    "        corr_coef[i,j] = np.corrcoef([a[mask],b[mask]])[0,1]\n",
    "\n",
    "plt.title('white - no corr., red - positive corr., blue - negative corr.')\n",
    "plt.imshow(corr_coef,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('features')\n",
    "plt.savefig('corr_coef.png')\n",
    "plt.close()\n",
    "\n",
    "# covariance matrix, masking out nans (didn't find a way to do this without for loops)\n",
    "cov = np.zeros([41,41])\n",
    "for i in range(41):\n",
    "    for j in range(41):\n",
    "        a = data[:,i+1]\n",
    "        b = data[:,j+1]\n",
    "        mask_a = np.isfinite(a)\n",
    "        mask_b = np.isfinite(b)\n",
    "        mask = mask_a & mask_b\n",
    "        cov[i,j] = np.cov([a[mask],b[mask]])[0,1]\n",
    "\n",
    "plt.imshow(cov,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('features')\n",
    "plt.savefig('covariance.png')\n",
    "plt.close()\n",
    "\n",
    "for i in range(41):\n",
    "    for j in range(i):\n",
    "        # i+1 and j+1 to skip the first row of classes\n",
    "        for k in range(5):\n",
    "            mask = (data[:,0] == k)\n",
    "            plt.plot(data[mask,i+1],data[mask,j+1],'+')\n",
    "        plt.xlabel(header[i])\n",
    "        plt.ylabel(header[j])\n",
    "        plt.savefig('imgs/'+header[i]+'-'+header[j]+'.png')\n",
    "        plt.close()\n",
    "        \n",
    "print 'done'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Class separability or class discriminatory power of the features.\n",
    "Calculate the Fisher's discriminant ratio for each feature and rank the features in descending order. The first feature in the list has the highest class separability of all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "standardize data...\n",
      "done\n",
      "calculate Fisher discriminant ratio (FDR)...\n",
      "done\n",
      "rank the features...\n",
      "   features sorted in order of how well they discriminate between different classes (first item is best):\n",
      "   ['ThetaPi_1' 'H2.H1_1' 'H1_1' 'Theta1Pi_1' 'H12_1' 'ThetaS_1' 'DAF_1'\n",
      " 'Theta1S_1' 'TajD_1' 'FuLiF_1' 'Theta1L_1' 'FuLiD_1' 'DXPEHH_12'\n",
      " 'FuLiF1_1' 'TajD1_1' 'DXPEHH_1' 'FuLiD1_1' 'FayWuH_1' 'XPEHH_12' 'FST_1'\n",
      " 'SL1_1' 'H2_1' 'XPEHH_13' 'ThetaL_1' 'SL0_1' 'iHH0_1' 'DDAF_1' 'Theta1H_1'\n",
      " 'Theta1Xi_1' 'nSL_1' 'iHS_1' 'FayWuH1_1' 'DnSL_1' 'iHH1_1' 'ZengE1_1'\n",
      " 'MAF_1' 'ZengE_1' 'ZA_1' 'ThetaXi_1' 'DiHH_1' 'ThetaH_1']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "standardize = True\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "            header = np.array(header)\n",
    "            header[-1] = header[-1][:-2]\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "if standardize:\n",
    "    print 'standardize data...'\n",
    "    # This is necessary because different features have different dynamic ranges. Standardization brings all features \n",
    "    # to the same scale.\n",
    "\n",
    "    mean = np.mean(data,axis=0)\n",
    "    std = np.std(data,axis=0,ddof=1)\n",
    "\n",
    "    data_standard = np.zeros(np.shape(data))*np.nan\n",
    "    data_mask = np.zeros(np.shape(data))\n",
    "    # save the classes\n",
    "    data_standard[:,0] = data[:,0]\n",
    "    data_mask = np.isfinite(data)\n",
    "    # standardize data\n",
    "    for i in range(41):\n",
    "        a = data[:,i+1]\n",
    "        mask_a = data_mask[:,i+1]\n",
    "        mean = np.mean(a[mask_a]) \n",
    "        std = np.std(a[mask_a],ddof=1)\n",
    "\n",
    "        data_standard[mask_a,i+1] = (a[mask_a] - mean)/std\n",
    "    \n",
    "    print 'done'\n",
    "\n",
    "    print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "    # FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "    # sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "    # The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "    # are better at distinguishing classes.\n",
    "    # For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "\n",
    "    FDR = np.zeros(41)\n",
    "    for i in range(41):\n",
    "        FDR_sum = 0e0\n",
    "        for j in range(5):\n",
    "            mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "            for k in range(j):\n",
    "                mask_k = data_mask[:,i+1] & (data_standard[:,0] == k)\n",
    "\n",
    "                mu_j = np.mean(data_standard[mask_j,i+1])\n",
    "                mu_k = np.mean(data_standard[mask_k,i+1])\n",
    "                sigma_j = np.var(data_standard[mask_j,i+1],ddof=1)\n",
    "                sigma_k = np.var(data_standard[mask_k,i+1],ddof=1)\n",
    "\n",
    "                FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "        # check the values     \n",
    "        #print i,FDR_sum\n",
    "        #for j in range(5):\n",
    "        #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "        FDR[i] = FDR_sum\n",
    "            \n",
    "else:\n",
    "    print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "    # FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "    # sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "    # The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "    # are better at distinguishing classes.\n",
    "    # For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "    \n",
    "    data_mask = np.zeros(np.shape(data))\n",
    "    data_mask = np.isfinite(data)\n",
    "    \n",
    "    FDR = np.zeros(41)\n",
    "    for i in range(41):\n",
    "        FDR_sum = 0e0\n",
    "        for j in range(5):\n",
    "            mask_j = data_mask[:,i+1] & (data[:,0] == j)\n",
    "            for k in range(j):\n",
    "                mask_k = data_mask[:,i+1] & (data[:,0] == k)\n",
    "\n",
    "                mu_j = np.mean(data[mask_j,i+1])\n",
    "                mu_k = np.mean(data[mask_k,i+1])\n",
    "                sigma_j = np.var(data[mask_j,i+1])\n",
    "                sigma_k = np.var(data[mask_k,i+1])\n",
    "\n",
    "                FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "        # check the values     \n",
    "        #print i,FDR_sum\n",
    "        #for j in range(5):\n",
    "        #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "        FDR[i] = FDR_sum\n",
    "    \n",
    "    \n",
    "print 'done'\n",
    "\n",
    "print 'rank the features...'\n",
    "indx_sorted = np.argsort(FDR)[::-1]\n",
    "print '   features sorted in order of how well they discriminate between different classes (first item is best):'\n",
    "print '  ',header[indx_sorted]\n",
    "\n",
    "# make the plot\n",
    "\n",
    "plt.plot(FDR[indx_sorted],'r+',markersize=10)\n",
    "plt.plot(FDR[indx_sorted],'k',linewidth=2)\n",
    "plt.ylabel('Fishers discriminat ratio')\n",
    "plt.xlabel('features')\n",
    "plt.xticks(range(41),header[indx_sorted],rotation = 90)\n",
    "plt.tight_layout(w_pad=0, h_pad=0)\n",
    "plt.savefig('FDR.png')\n",
    "plt.close()\n",
    "\n",
    "# save data in csv\n",
    "with open('data_files/FDR.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(41):\n",
    "        writer.writerow([header[indx_sorted[i]],FDR[indx_sorted[i]]])\n",
    "\n",
    "for n in range(41):\n",
    "    corr_coef = np.zeros([n+1,n+1])\n",
    "    for i in range(n+1):\n",
    "        for j in range(n+1):\n",
    "            a = data[:,indx_sorted[i]]\n",
    "            b = data[:,indx_sorted[j]]\n",
    "            mask_a = np.isfinite(a)\n",
    "            mask_b = np.isfinite(b)\n",
    "            mask = mask_a & mask_b\n",
    "            corr_coef[i,j] = np.corrcoef([a[mask],b[mask]])[0,1]\n",
    "            \n",
    "    np.savetxt('data_files/corr_coef_'+str(n+1)+'.csv', corr_coef, delimiter=\",\")\n",
    "    \n",
    "# add 1 and insert 0 to the first place to keep the class in.\n",
    "indx_sorted = np.insert(indx_sorted+1,0,0)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use an SVM to do classification\n",
    "separate the data into training, cross validation, and test data sets (60-20-20%).\n",
    "make a loop through successively less features:\n",
    "    - use all features to find the best values for C and gamma in the SVM\n",
    "    - fix the best C and gamma values and successively remove a feature that is least discriminative\n",
    "    - check what number of features give the best score in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the SVM and find the best C and gamma parameters...\n",
      "   round 0\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.428571428571\n",
      "   round 1\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.583333333333\n",
      "   round 2\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.542168674699\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.559523809524\n",
      "   round 3\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.511904761905\n",
      "   round 4\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.47619047619\n",
      "   round 5\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.66265060241\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.559523809524\n",
      "   round 6\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.535714285714\n",
      "   round 7\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.595238095238\n",
      "   round 8\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.619047619048\n",
      "   round 9\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.571428571429\n",
      "   round 10\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.464285714286\n",
      "   round 11\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.566265060241\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.559523809524\n",
      "   round 12\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.595238095238\n",
      "   round 13\n",
      "      best C value(s): [   1.   10.   10.  100.]\n",
      "      best gamma value(s): [ 0.01   0.001  0.01   0.001]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.535714285714\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.595238095238\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.619047619048\n",
      "   round 14\n",
      "      best C value(s): [   10.    10.   100.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-03   1.00000000e-02   1.00000000e-04   1.00000000e-05]\n",
      "      max test score: 0.530120481928\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.583333333333\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.642857142857\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.583333333333\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.583333333333\n",
      "   round 15\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.630952380952\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.630952380952\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.630952380952\n",
      "   round 16\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.535714285714\n",
      "   round 17\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.518072289157\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.607142857143\n",
      "   round 18\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.5\n",
      "   round 19\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.686746987952\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.52380952381\n",
      "   round 20\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-04   1.00000000e-05]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.535714285714\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.535714285714\n",
      "   round 21\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.638554216867\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.547619047619\n",
      "   round 22\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.674698795181\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.595238095238\n",
      "   round 23\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [ 0.001   0.0001]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.488095238095\n",
      "   round 24\n",
      "      best C value(s): [  1.  10.]\n",
      "      best gamma value(s): [ 0.01   0.001]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.666666666667\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.630952380952\n",
      "   round 25\n",
      "      best C value(s): [   10.  1000.]\n",
      "      best gamma value(s): [ 0.01    0.0001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.5\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.511904761905\n",
      "   round 26\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.506024096386\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.488095238095\n",
      "   round 27\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.619047619048\n",
      "   round 28\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.571428571429\n",
      "   round 29\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.542168674699\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.535714285714\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.535714285714\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.535714285714\n",
      "   round 30\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.583333333333\n",
      "   round 31\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.518072289157\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.559523809524\n",
      "   round 32\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.559523809524\n",
      "   round 33\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.686746987952\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.583333333333\n",
      "   round 34\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.638554216867\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.666666666667\n",
      "   round 35\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.511904761905\n",
      "   round 36\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.595238095238\n",
      "   round 37\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.5\n",
      "   round 38\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.542168674699\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.630952380952\n",
      "   round 39\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.5\n",
      "   round 40\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.566265060241\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.547619047619\n",
      "   round 41\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.47619047619\n",
      "   round 42\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.571428571429\n",
      "   round 43\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-04   1.00000000e-05]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.595238095238\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.595238095238\n",
      "   round 44\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.559523809524\n",
      "   round 45\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [ 0.01    0.0001]\n",
      "      max test score: 0.530120481928\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.583333333333\n",
      "   round 46\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.638554216867\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.52380952381\n",
      "   round 47\n",
      "      best C value(s): [ 10.  10.]\n",
      "      best gamma value(s): [ 0.001  0.01 ]\n",
      "      max test score: 0.530120481928\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.595238095238\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.571428571429\n",
      "   round 48\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.595238095238\n",
      "   round 49\n",
      "      best C value(s): [   10.   100.  1000.  1000.]\n",
      "      best gamma value(s): [ 0.1   0.1   0.01  0.1 ]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.559523809524\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.547619047619\n",
      "   round 50\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.1]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=1.0 and gamma=0.1:\n",
      "          0.583333333333\n",
      "   round 51\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [ 0.001  0.001]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.571428571429\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.488095238095\n",
      "   round 52\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.452380952381\n",
      "   round 53\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.595238095238\n",
      "   round 54\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.535714285714\n",
      "   round 55\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.583333333333\n",
      "   round 56\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.511904761905\n",
      "   round 57\n",
      "      best C value(s): [  100.   100.  1000.  1000.]\n",
      "      best gamma value(s): [ 0.001   0.01    0.0001  0.01  ]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.452380952381\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.464285714286\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.47619047619\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.5\n",
      "   round 58\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.488095238095\n",
      "   round 59\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.642857142857\n",
      "   round 60\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.452380952381\n",
      "   round 61\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.547619047619\n",
      "   round 62\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.47619047619\n",
      "   round 63\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.530120481928\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.702380952381\n",
      "   round 64\n",
      "      best C value(s): [   10.    10.   100.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-03   1.00000000e-02   1.00000000e-04   1.00000000e-05]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.52380952381\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.488095238095\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.52380952381\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.52380952381\n",
      "   round 65\n",
      "      best C value(s): [  1.   1.  10.]\n",
      "      best gamma value(s): [ 0.01  0.1   0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=1.0 and gamma=0.1:\n",
      "          0.535714285714\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.619047619048\n",
      "   round 66\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.559523809524\n",
      "   round 67\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.493975903614\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.607142857143\n",
      "   round 68\n",
      "      best C value(s): [   10.   100.  1000.  1000.]\n",
      "      best gamma value(s): [ 0.1   0.1   0.01  0.1 ]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.583333333333\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.583333333333\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.583333333333\n",
      "   round 69\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.710843373494\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.619047619048\n",
      "   round 70\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.571428571429\n",
      "   round 71\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-04   1.00000000e-05]\n",
      "      max test score: 0.638554216867\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.547619047619\n",
      "   round 72\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.542168674699\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "   round 73\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.511904761905\n",
      "   round 74\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.559523809524\n",
      "   round 75\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.654761904762\n",
      "   round 76\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.595238095238\n",
      "   round 77\n",
      "      best C value(s): [  10.  100.]\n",
      "      best gamma value(s): [ 0.01  0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.583333333333\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.559523809524\n",
      "   round 78\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.01    0.001   0.0001]\n",
      "      max test score: 0.66265060241\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.5\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.511904761905\n",
      "   round 79\n",
      "      best C value(s): [  10.  100.]\n",
      "      best gamma value(s): [ 0.01   0.001]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.607142857143\n",
      "      the cross validation score for C=100.0 and gamma=0.001:\n",
      "          0.571428571429\n",
      "   round 80\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.638554216867\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.452380952381\n",
      "   round 81\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.674698795181\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.452380952381\n",
      "   round 82\n",
      "      best C value(s): [    1.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-01   1.00000000e-05]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=1.0 and gamma=0.1:\n",
      "          0.571428571429\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.619047619048\n",
      "   round 83\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.607142857143\n",
      "   round 84\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.626506024096\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.535714285714\n",
      "   round 85\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.5\n",
      "   round 86\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.583333333333\n",
      "   round 87\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "   round 88\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.535714285714\n",
      "   round 89\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.638554216867\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.511904761905\n",
      "   round 90\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.66265060241\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.428571428571\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.428571428571\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.428571428571\n",
      "   round 91\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.001]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=1000.0 and gamma=0.001:\n",
      "          0.547619047619\n",
      "   round 92\n",
      "      best C value(s): [   10.    10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.01  0.1   0.1   0.1 ]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.511904761905\n",
      "   round 93\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.650602409639\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.547619047619\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.547619047619\n",
      "   round 94\n",
      "      best C value(s): [   10.  1000.]\n",
      "      best gamma value(s): [ 0.01    0.0001]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=10.0 and gamma=0.01:\n",
      "          0.571428571429\n",
      "      the cross validation score for C=1000.0 and gamma=0.0001:\n",
      "          0.547619047619\n",
      "   round 95\n",
      "      best C value(s): [  100.  1000.]\n",
      "      best gamma value(s): [ 0.01  0.01]\n",
      "      max test score: 0.602409638554\n",
      "      the cross validation score for C=100.0 and gamma=0.01:\n",
      "          0.464285714286\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.488095238095\n",
      "   round 96\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.566265060241\n",
      "      the cross validation score for C=1000.0 and gamma=0.01:\n",
      "          0.488095238095\n",
      "   round 97\n",
      "      best C value(s): [   10.   100.  1000.]\n",
      "      best gamma value(s): [ 0.1  0.1  0.1]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=10.0 and gamma=0.1:\n",
      "          0.488095238095\n",
      "      the cross validation score for C=100.0 and gamma=0.1:\n",
      "          0.488095238095\n",
      "      the cross validation score for C=1000.0 and gamma=0.1:\n",
      "          0.488095238095\n",
      "   round 98\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.01]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1.0 and gamma=0.01:\n",
      "          0.452380952381\n",
      "   round 99\n",
      "      best C value(s): [  10.  100.]\n",
      "      best gamma value(s): [ 0.001   0.0001]\n",
      "      max test score: 0.566265060241\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.595238095238\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.642857142857\n",
      "done\n",
      "The best C and gamma values are 10.0 and 0.01 , respectively.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:27: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:28: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:30: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:31: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:33: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:34: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# run the previous cell first!\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# do everything 100 times to characterize the noise\n",
    "\n",
    "print 'Train the SVM and find the best C and gamma parameters...'\n",
    "C_collect = []\n",
    "gamma_collect = []\n",
    "for i in range(100):\n",
    "    print '   round',i\n",
    "\n",
    "    # I can only use those data points that do not contain NAs\n",
    "    mask_to_use = np.all(np.isfinite(data),axis=1)\n",
    "    \n",
    "    if standardize:\n",
    "        data_to_use = data_standard[mask_to_use,:]\n",
    "    else:\n",
    "        data_to_use = data[mask_to_use,:]\n",
    "    \n",
    "    # nr of data points left\n",
    "    n = np.shape(data_to_use)[0]\n",
    "    \n",
    "    # shuffle and divide up the data\n",
    "    indx = np.arange(n)\n",
    "    np.random.shuffle(indx)\n",
    "    \n",
    "    X_train = data_to_use[indx[:n*0.6],1:]\n",
    "    Y_train = data_to_use[indx[:n*0.6],0]\n",
    "    \n",
    "    X_test = data_to_use[indx[n*0.6:n*0.8],1:]\n",
    "    Y_test = data_to_use[indx[n*0.6:n*0.8],0]\n",
    "    \n",
    "    X_CV = data_to_use[indx[n*0.8:],1:]\n",
    "    Y_CV = data_to_use[indx[n*0.8:],0]\n",
    "    \n",
    "    # parameter ranges for the SVM\n",
    "    C = 10e0**(np.linspace(-8e0,3e0,12))\n",
    "    gamma = 10e0**(np.linspace(-5e0,6e0,12))\n",
    "    \n",
    "    # arrays to store scores and best parameters\n",
    "    train_score = np.zeros([len(C),len(gamma)])\n",
    "    test_score = np.zeros([len(C),len(gamma)])\n",
    "    C_array = np.zeros([len(C),len(gamma)])\n",
    "    gamma_array = np.zeros([len(C),len(gamma)])\n",
    "    # loop through the C and gamma arrays\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(gamma)):\n",
    "            SVM = SVC(kernel='rbf', C=C[i], gamma=gamma[j]).fit(X_train, Y_train)\n",
    "            train_score[i,j] = SVM.score(X_train,Y_train)\n",
    "            test_score[i,j] = SVM.score(X_test,Y_test)\n",
    "            C_array[i,j] = C[i]\n",
    "            gamma_array[i,j] = gamma[j]\n",
    "\n",
    "    # find the C and gamma parameters that give max score.\n",
    "    # if there are multiple parameter configuration giving max score, the first one of these is used below \n",
    "    best_params = np.where(test_score == np.max(test_score))\n",
    "    print '      best C value(s):',C[best_params[0]]\n",
    "    print '      best gamma value(s):',gamma[best_params[1]]\n",
    "    print '      max test score:',np.max(test_score)\n",
    "    # calculate the cross validation score\n",
    "    for j in range(len(best_params[0])):\n",
    "        print '      the cross validation score for C='+str(C[best_params[0][j]])+' and gamma='+str(gamma[best_params[1][j]])+':'\n",
    "        SVM = SVC(kernel='rbf', C=C[best_params[0][j]], gamma=gamma[best_params[1][j]]).fit(X_train, Y_train)\n",
    "        CV_score = SVM.score(X_CV,Y_CV)\n",
    "        print '         ',CV_score\n",
    "    \n",
    "    C_collect.append(C[best_params[0]])\n",
    "    gamma_collect.append(gamma[best_params[1]])\n",
    "\n",
    "print 'done'\n",
    "\n",
    "# flatten the *_collect lists and find the most frequently occuring elements\n",
    "C_flat = [item for sublist in C_collect for item in sublist]\n",
    "gamma_flat = [item for sublist in gamma_collect for item in sublist]\n",
    "\n",
    "combined = [t for t in zip(C_flat,gamma_flat)]\n",
    "(C, gamma) = max(set(combined), key=combined.count)\n",
    "\n",
    "print 'The best C and gamma values are',C,'and',gamma,', respectively.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove one feature at a time (the least discriminative one) and calculate the classification score...\n",
      "    0\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:28: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:31: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:32: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print 'Remove one feature at a time (the least discriminative one) and calculate the classification score...'\n",
    "\n",
    "n_sim = 10\n",
    "train_score = np.zeros([n_sim,40])\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "for i in range(n_sim):\n",
    "    if i%10 == 0:\n",
    "        print '   ',i\n",
    "    for j in range(40):        \n",
    "        # I can only use those data points that do not contain NAs\n",
    "        mask_to_use = np.all(np.isfinite(data[:,indx_sorted[:-(j+1)]]),axis=1)\n",
    "        \n",
    "        if standardize:\n",
    "            data_to_use = data_standard[:,indx_sorted[:-(j+1)]]  \n",
    "        else:\n",
    "            data_to_use = data[:,indx_sorted[:-(j+1)]]\n",
    "        \n",
    "        data_to_use = data_to_use[mask_to_use,:]\n",
    "                        \n",
    "        # nr of data points left\n",
    "        n = np.shape(data_to_use)[0]\n",
    "                \n",
    "        # shuffle and divide up the data\n",
    "        indx = np.arange(n)\n",
    "        np.random.shuffle(indx)\n",
    "\n",
    "        X_train = data_to_use[indx[:n*0.75],1:]\n",
    "        Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "        X_test = data_to_use[indx[n*0.75:],1:]\n",
    "        Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "        SVM = SVC(kernel='rbf', C=C, gamma=gamma).fit(X_train, Y_train)\n",
    "        train_score[i,j] = SVM.score(X_train,Y_train)\n",
    "        test_score[i,j] = SVM.score(X_test,Y_test)\n",
    "                \n",
    "\n",
    "# make a plot            \n",
    "plt.close()\n",
    "plt.ylim([0,80])\n",
    "plt.errorbar(41 - (np.arange(40)+1),np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "plt.xlabel('nr. of features used')\n",
    "plt.ylabel('test score [%]')\n",
    "if standardize:\n",
    "    plt.savefig('SVM_standardized_'+str(n_sim)+'.png')\n",
    "else:\n",
    "    plt.savefig('SVM_'+str(n_sim)+'.png')\n",
    "plt.close()\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gaussian naive bayes classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:19: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:20: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:25: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:26: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [  1 900]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-124d6c9917d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[1;32m    175\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[0;32m--> 176\u001b[0;31m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [  1 900]"
     ]
    }
   ],
   "source": [
    "# trying a naive bayes classifier of sklearn\n",
    "# it does not handle missing data!\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if standardize:\n",
    "    data_to_use = data_standard\n",
    "else:\n",
    "    data_to_use = data\n",
    "\n",
    "data_mask = np.zeros(np.shape(data))\n",
    "data_mask = np.isfinite(data)\n",
    "    \n",
    "# shuffle and divide up the data\n",
    "n = np.shape(data_to_use)[0]\n",
    "indx = np.arange(n)\n",
    "np.random.shuffle(indx)\n",
    "\n",
    "X_train = data_to_use[indx[:n*0.6],1:]\n",
    "Y_train = data_to_use[indx[:n*0.6],0]\n",
    "\n",
    "X_test = data_to_use[indx[n*0.6:n*0.8],1:]\n",
    "Y_test = data_to_use[indx[n*0.6:n*0.8],0]\n",
    "\n",
    "X_CV = data_to_use[indx[n*0.8:],1:]\n",
    "Y_CV = data_to_use[indx[n*0.8:],0]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train[np.isfinite(X_train)], Y_train).predict(X_test[np.isfinite(X_test)])\n",
    "\n",
    "print (Y_test != y_pred).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:43: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:45: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:46: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# gaussian naive bayes classifier from scratch that handles missing data\n",
    "# not optimized!\n",
    "# run the FDR ranking cell first!\n",
    "\n",
    "n_sim = 100\n",
    "\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "# run n_sim different simulations to get a feeling of random effects\n",
    "for ii in range(n_sim):\n",
    "\n",
    "    if ii%10 == 0:\n",
    "        print ii\n",
    "    \n",
    "    # remove one feature at a time (the least disciminative one)\n",
    "    for jj in range(40):\n",
    "\n",
    "        if standardize:\n",
    "            data_to_use = data_standard[:,indx_sorted[:-(jj+1)]]\n",
    "        else:\n",
    "            data_to_use = data[:,indx_sorted[:-(jj+1)]] \n",
    "\n",
    "        \n",
    "        # arrays used to summarize data\n",
    "        # size of arrays: [nr of classes, nr of features]\n",
    "        nr_classes = len(np.unique(data_to_use[:,0]))\n",
    "        nr_features = np.shape(data_to_use[:,1:])[1]\n",
    "\n",
    "        mean = np.zeros([nr_classes,nr_features])\n",
    "        std = np.zeros(np.shape(mean))\n",
    "        # correction factor for the probabilities: its value is the fraction of data points used to calculate the mean and std\n",
    "        #   1 if all points are used, 0.1 if only 10% of points are used\n",
    "        corr_factor = np.zeros(np.shape(mean))\n",
    "        # the probability that the point is drawn from class i\n",
    "        P_C = np.zeros(nr_classes)\n",
    "\n",
    "        # shuffle and divide up the data\n",
    "        n = np.shape(data_to_use)[0]\n",
    "        indx = np.arange(n)\n",
    "        np.random.shuffle(indx)\n",
    "\n",
    "        X_train = data_to_use[indx[:n*0.75],1:]\n",
    "        Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "        X_test = data_to_use[indx[n*0.75:],1:]\n",
    "        Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "\n",
    "        # fill up the mean, std, and corr_factor arrays\n",
    "        for i in range(nr_classes):\n",
    "            mask = Y_train == i\n",
    "            points_in_class = X_train[mask,:]\n",
    "            P_C[i] = float(np.sum(mask)) / float(len(mask))\n",
    "            for j in range(nr_features):\n",
    "                feature_j = points_in_class[:,j]\n",
    "                mask = np.isfinite(feature_j)\n",
    "\n",
    "                mean[i,j] = np.mean(feature_j[mask])\n",
    "                std[i,j] = np.std(feature_j[mask])\n",
    "                corr_factor[i,j] = float(np.sum(mask)) / float(len(feature_j))\n",
    "\n",
    "        # loop through the test points and estimate the most likely class\n",
    "        score = 0e0\n",
    "        for i in range(len(Y_test)): \n",
    "            \n",
    "            class_prob = np.zeros(nr_classes)\n",
    "            for j in range(nr_classes):\n",
    "                \n",
    "                # calculate the log probabilities that this data point was sampled from class j\n",
    "                # adding log probabilities is better than multiplying probabilities \n",
    "                log_factor = np.log(corr_factor[j,:] / (np.sqrt(2e0*np.pi)*std[j,:]))\n",
    "                log_exponent = -((X_test[i,:]-mean[j,:])**2e0/(2e0*std[j,:]**2e0))\n",
    "                log_prob = log_factor + log_exponent\n",
    "                class_prob[j] = np.log(P_C[j])+np.sum(log_prob[np.isfinite(log_prob)])\n",
    "                \n",
    "            if np.argmax(class_prob) == Y_test[i]:\n",
    "                score = score + 1\n",
    "        \n",
    "        test_score[ii,jj] = score / len(Y_test)\n",
    "\n",
    "# make a plot            \n",
    "plt.close()\n",
    "plt.ylim([0,80])\n",
    "plt.errorbar(41 - (np.arange(40)+1),np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "plt.xlabel('nr. of features used')\n",
    "plt.ylabel('test score [%]')\n",
    "if standardize:\n",
    "    plt.savefig('gaussian_naive_bayes_standardized_'+str(n_sim)+'.png')\n",
    "else:\n",
    "    plt.savefig('gaussian_naive_bayes_'+str(n_sim)+'.png')\n",
    "plt.close()\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardize: True\n",
      "0.01\n",
      "    0\n",
      "[ 0.2         0.19822222  0.20177778  0.19733333  0.20266667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:47: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:48: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:50: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:51: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-421525b9673f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mkernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernels_class_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mP_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# loop through the test points and estimate the most likely class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# general naive bayes classifier\n",
    "# the true probability distribution of the features is estimated using a gaussian kernel density estimator\n",
    "# https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "# scipy's kde is fastest for a few 100 data points.\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "n_sim = 100\n",
    "\n",
    "# test what bandwidth value gives best scores\n",
    "bandwidth = 10e0**(np.linspace(-2,0,num=11,endpoint = True))\n",
    "\n",
    "print 'standardize:',standardize\n",
    "\n",
    "for bw in bandwidth:\n",
    "    print bw\n",
    "    \n",
    "    test_score = np.zeros([n_sim,41])\n",
    "    conf_mat = np.zeros([41,5,5])\n",
    "\n",
    "    final_class_count_true = np.zeros(5)\n",
    "    final_class_count_pred = np.zeros(5)\n",
    "    \n",
    "    inf_count = 0\n",
    "    \n",
    "    # run n_sim different simulations to get a feeling of random effects\n",
    "    for ii in range(n_sim):\n",
    "        if ii%10 == 0:\n",
    "            print '   ',ii\n",
    "\n",
    "        # remove one feature at a time (the least disciminative one)\n",
    "        for jj in range(41)[::-1]:\n",
    "            if standardize:\n",
    "                data_to_use = data_standard[:,indx_sorted[:jj+2]]\n",
    "            else:\n",
    "                data_to_use = data[:,indx_sorted[:jj+2]] \n",
    "\n",
    "            nr_classes = len(np.unique(data_to_use[:,0]))\n",
    "            nr_features = np.shape(data_to_use[:,1:])[1]            \n",
    "\n",
    "            # shuffle and divide up the data\n",
    "            n = np.shape(data_to_use)[0]\n",
    "            indx = np.arange(n)\n",
    "            np.random.shuffle(indx)\n",
    "\n",
    "            X_train = data_to_use[indx[:n*0.75],1:]\n",
    "            Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "            X_test = data_to_use[indx[n*0.75:],1:]\n",
    "            Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "\n",
    "            # collect kernels for each class and feature\n",
    "            kernels = []\n",
    "            # the probability that the point is drawn from class i\n",
    "            P_C = np.zeros(nr_classes)\n",
    "            # correction factor, what fraction of the data points were used to estimate the kernel\n",
    "            # 1 if all points are used, 0.1 if 10% of points are used\n",
    "            corr_factor = np.zeros([nr_classes,nr_features])\n",
    "            \n",
    "            for i in range(nr_classes):\n",
    "                mask = Y_train == i\n",
    "                points_in_class = X_train[mask,:]\n",
    "                \n",
    "                P_C[i] = float(np.sum(mask)) / float(len(Y_train))\n",
    "                \n",
    "                kernels_class_i = []\n",
    "                \n",
    "                for j in range(nr_features):\n",
    "                    feature_j = points_in_class[:,j]\n",
    "                    mask = np.isfinite(feature_j)\n",
    "                    \n",
    "                    corr_factor[i,j] = float(np.sum(mask)) / float(len(feature_j))\n",
    "                    kernels_class_i.append(gaussian_kde(feature_j[mask],bw_method = bw))\n",
    "                    \n",
    "                kernels.append(kernels_class_i)\n",
    "\n",
    "            # loop through the test points and estimate the most likely class\n",
    "            score = 0e0\n",
    "            Y_pred = np.zeros(len(Y_test))\n",
    "            for i in range(len(Y_test)): \n",
    "\n",
    "                class_prob = np.zeros(nr_classes)\n",
    "                for j in range(nr_classes):\n",
    "                    # mask NA values\n",
    "                    mask = np.isfinite(X_test[i,:])                    \n",
    "                    kernels_class_j = [kernel for (kernel, m) in zip(kernels[j],mask) if m]\n",
    "                    class_prob[j] = np.log(P_C[j]) + np.sum(np.log(corr_factor[j,mask]) + [kernel.logpdf(point) for kernel, point in zip(kernels_class_j, X_test[i,mask])])\n",
    "                \n",
    "                Y_pred[i] = np.argmax(class_prob)\n",
    "                if np.argmax(class_prob) == Y_test[i]:\n",
    "                    score = score + 1\n",
    "                \n",
    "                final_class_count_pred[np.argmax(class_prob)] = final_class_count_pred[np.argmax(class_prob)] + 1\n",
    "                final_class_count_true[Y_test[i]] = final_class_count_true[Y_test[i]] + 1\n",
    "                \n",
    "                if np.all(np.isfinite(class_prob) == False):\n",
    "                    inf_count = inf_count + 1\n",
    "                \n",
    "            test_score[ii,jj] = score / len(Y_test)\n",
    "            \n",
    "            conf_mat[jj,:,:] = conf_mat[jj,:,:] + confusion_matrix(Y_test,Y_pred)\n",
    "    \n",
    "    # write the csv file    \n",
    "    for jj in range(41)[::-1]:\n",
    "        # generate histo values\n",
    "        hist, bin_edges = np.histogram(test_score[:,jj]*100e0,bins=100,range=(0,100))\n",
    "        bin_midpoints = (bin_edges[1:]+bin_edges[0:-1])/2e0\n",
    "        with open('data_files/histogram_nrf'+str(jj+1)+'_bw'+str(np.around(bw,2))+'.csv', 'wb') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for nn in range(100):\n",
    "                writer.writerow([bin_midpoints[nn],hist[nn]])\n",
    "        \n",
    "        # normalize and save the confusion matrix\n",
    "        np.savetxt('data_files/confusion_m_nrf'+str(jj+1)+'_bw'+str(np.around(bw,2))+'.csv', conf_mat[jj,:,:] / np.sum(conf_mat[jj,:,:]), delimiter=\",\")\n",
    "    \n",
    "    print final_class_count_pred / np.sum(final_class_count_pred)\n",
    "    print final_class_count_true / np.sum(final_class_count_true)\n",
    "    print inf_count\n",
    "    # make a plot            \n",
    "    plt.close()\n",
    "    plt.ylim([0,80])\n",
    "    plt.errorbar(np.arange(41)+1,np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "    plt.xlabel('nr. of features used')\n",
    "    plt.ylabel('test score [%]')\n",
    "    if standardize:\n",
    "        plt.savefig('general_naive_bayes_standardized_bw'+str(np.around(bw,2))+'_'+str(n_sim)+'.png')\n",
    "    else:\n",
    "        plt.savefig('general_naive_bayes_bw'+str(bw)+'_'+str(n_sim)+'.png')\n",
    "    plt.close()\n",
    "    print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rewrite general naive bayes classifier using pandas\n",
    "# the true probability distribution of the features is estimated using a gaussian kernel density estimator\n",
    "# https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "# scipy's kde is fastest for a few 100 data points.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv('data-hl.txt', sep='\\t')\n",
    "files = ['data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "for f in files:\n",
    "    df = pd.concat([df, pd.read_csv(f, sep='\\t')])\n",
    "\n",
    "# calculate correlation coefficient and make figure\n",
    "#corr_coef = df.corr()\n",
    "#plt.title('white - no corr., red - positive corr., blue - negative corr.')\n",
    "#plt.imshow(corr_coef,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "#plt.xlabel('features')\n",
    "#plt.ylabel('features')\n",
    "#plt.savefig('corr_coef.png')\n",
    "#plt.close()\n",
    "\n",
    "# separate class from features\n",
    "Y = df['sclass']\n",
    "classes = np.unique(Y)\n",
    "nr_classes = len(classes)\n",
    "for i in range(len(classes)):\n",
    "    Y.replace(classes[i],i,inplace = True)\n",
    "X = df.drop('sclass',1)\n",
    "feature_names = np.array(X.columns.values)\n",
    "\n",
    "# standardize data\n",
    "# NOTE: numpy and pandas standardize in different ways, results slightly differ:\n",
    "# http://stackoverflow.com/questions/24984178/different-std-in-pandas-vs-numpy\n",
    "X_standard = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# calculate Fisher's discriminant rato\n",
    "mean = np.array([X_standard[Y == i].mean() for i in range(len(classes))])\n",
    "var = np.array([X_standard[Y == i].var() for i in range(len(classes))])\n",
    "FDR = np.sum([(mean[i,:]-mean[j,:])**2e0/(var[i,:]**2e0+var[j,:]**2e0) \\\n",
    "              for i in range(len(classes)) for j in range(i)],axis=0)\n",
    "\n",
    "# reorder columns in X according to FDR\n",
    "indx_sorted = np.argsort(FDR)[::-1]\n",
    "X = X[feature_names[indx_sorted]]\n",
    "\n",
    "# do the classification\n",
    "n_sim = 10\n",
    "bandwidth = 10e0**(np.linspace(-2,0,num=11,endpoint = True))\n",
    "\n",
    "# test score - what fraction of points were correctly classified\n",
    "test_score = np.zeros([len(bandwidth),len(feature_names),n_sim])\n",
    "\n",
    "for i in range(len(bandwidth)):\n",
    "    for j in range(len(feature_names))[::-1]:\n",
    "        \n",
    "        # remove the least discriminative feature\n",
    "        X_use = X[feature_names[indx_sorted[:j+1]]]\n",
    "        # confusion matrix\n",
    "        conf_m = np.zeros([nr_classes,nr_classes])\n",
    "\n",
    "        # perform n_sim runs\n",
    "        for k in range(n_sim):\n",
    "            \n",
    "            # split X_used into training and test sets\n",
    "            split = np.random.rand(len(X_use)) < 0.75\n",
    "            X_train = X_use[split]\n",
    "            Y_train = Y[split]\n",
    "            \n",
    "            X_test = X_use[~split]\n",
    "            Y_test = Y[~split]\n",
    "            \n",
    "            # collect kernels for each class and feature\n",
    "            kernels = []\n",
    "            # the probability that the point is drawn from class i\n",
    "            P_C = np.zeros(nr_classes)\n",
    "            # correction factor, what fraction of the data points were used to estimate the kernel\n",
    "            # 1 if all points are used, 0.1 if 10% of points are used\n",
    "            corr_factor = np.zeros([nr_classes,j+1])\n",
    "            \n",
    "            for ii in range(nr_classes):\n",
    "                # get points in this class\n",
    "                points_in_class = X_train[Y_train==ii]\n",
    "                # claculate what fraction of points are in this class\n",
    "                P_C[ii] = 1e0*Y_train[Y_train==ii].count() / Y_train.count()\n",
    "                # collect kernels in this class\n",
    "                kernels_class_i = []\n",
    "                for jj in range(j+1):\n",
    "                    # get the feature\n",
    "                    feature = points_in_class[feature_names[indx_sorted[jj]]]\n",
    "                    # calculate the correction factor\n",
    "                    corr_factor[ii,jj] = 1e0*feature.notnull().sum().sum() / len(feature)\n",
    "                    # calculate kernels\n",
    "                    kernels_class_i.append(gaussian_kde(feature.dropna(),bw_method = bandwidth[i]))\n",
    "                    \n",
    "                kernels.append(kernels_class_i)\n",
    "            \n",
    "            # loop through the test points and estimate the most likely class\n",
    "            score = 0e0\n",
    "            Y_pred = np.zeros(len(Y_test))\n",
    "            for ii in range(len(Y_test)): \n",
    "                # get the data point\n",
    "                test_point = X_test.iloc[ii]\n",
    "                #calculate class probabilities\n",
    "                class_prob = np.zeros(nr_classes)\n",
    "                for jj in range(nr_classes):\n",
    "                    # mask NA values\n",
    "                    mask = test_point.notnull()\n",
    "                    # collect kernels where features are not NA\n",
    "                    kernels_class_j = [kernel for (kernel, m) in zip(kernels[jj],mask) if m]\n",
    "                    # calculate class probability\n",
    "                    # log(class_prob) = log(P_C) + sum(log(corr_factor) + log(KDE_estimates))                    \n",
    "                    class_prob[jj] = np.log(P_C[jj]) + np.sum(np.log(corr_factor[jj,np.array(mask)]) + \\\n",
    "                        [kernel.logpdf(point) for kernel, point in zip(kernels_class_j, test_point[mask])])\n",
    "                # the prediction\n",
    "                Y_pred[ii] = np.argmax(class_prob)\n",
    "                # update the score\n",
    "                if Y_pred[ii] == Y_test.iloc[ii]:\n",
    "                    score = score + 1\n",
    "            \n",
    "            test_score[i,j,k] = 1e0*score/len(Y_test)\n",
    "            conf_m = conf_m + confusion_matrix(np.array(Y_test),Y_pred)\n",
    "                            \n",
    "        # save the confusion matrix\n",
    "        #np.savetxt('data_files/confusion_m_nrf'+str(j+1)+'_bw'+str(np.around(bandwidth[i],2))+'.csv', conf_m / np.sum(conf_m), delimiter=\",\")\n",
    "        # save the histogram\n",
    "        #hist, bin_edges = np.histogram(test_score*100e0,bins=100,range=(0,100))\n",
    "        #bin_midpoints = (bin_edges[1:]+bin_edges[0:-1])/2e0\n",
    "        #np.savetxt('data_files/histogram_nrf'+str(j+1)+'_bw'+str(np.around(bandwidth[i],2))+'.csv', np.c_[bin_midpoints,hist], delimiter=\",\")\n",
    "    \n",
    "    # make a plot\n",
    "    plt.close()\n",
    "    plt.ylim([0,80])\n",
    "    plt.errorbar(np.arange(41)+1,np.average(test_score[i,:,:],axis=1)*100e0,yerr=np.std(test_score[i,:,:],axis=1)*100,fmt='o')\n",
    "    plt.xlabel('nr. of features used')\n",
    "    plt.ylabel('test score [%]')\n",
    "    plt.savefig('general_naive_bayes_standardized_bw'+str(np.around(bandwidth[i],2))+'_'+str(n_sim)+'.png')\n",
    "    plt.close()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "## calculate corr_factor with list comprehensions\n",
    "## difficult to read\n",
    "# collect points used for KDE\n",
    "#points_for_KDE = []\n",
    "#[points_for_KDE.append(X_train.loc[Y_train==ii,feature_names[indx_sorted[:i+1]]]) for ii in range(len(classes))]\n",
    "# calculate the correction factor\n",
    "# all elements\n",
    "#all_elements = np.fromiter([len(points_for_KDE[ii][feature_names[indx_sorted[jj]]]) \\\n",
    "#    for ii in range(len(classes)) for jj in range(i+1)],dtype=float).reshape([len(classes),i+1])\n",
    "# non-Nan elements\n",
    "#non_NAN = np.fromiter([points_for_KDE[ii][feature_names[indx_sorted[jj]]].notnull().sum().sum() \\\n",
    "#    for ii in range(len(classes)) for jj in range(i+1)],dtype=float).reshape([len(classes),i+1])\n",
    "#corr_factor = non_NAN/all_elements\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
