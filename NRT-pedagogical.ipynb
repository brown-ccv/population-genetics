{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation coefficient matrix and the cloud of points for every pair of variables. This helps to familiarize myself with the data, it allows me to check how strongly correlated some variables are and what type of correlation is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "prepare the plots...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "print 'prepare the plots...'\n",
    "\n",
    "# correlation coefficient matrix, masking out nans (didn't find a way to do this without for loops)\n",
    "corr_coef = np.zeros([41,41])\n",
    "for i in range(41):\n",
    "    for j in range(41):\n",
    "        a = data[:,i+1]\n",
    "        b = data[:,j+1]\n",
    "        mask_a = np.isfinite(a)\n",
    "        mask_b = np.isfinite(b)\n",
    "        mask = mask_a & mask_b\n",
    "        corr_coef[i,j] = np.corrcoef([a[mask],b[mask]])[0,1]\n",
    "\n",
    "plt.title('white - no corr., red - positive corr., blue - negative corr.')\n",
    "plt.imshow(corr_coef,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "plt.savefig('corr_coef.png')\n",
    "plt.close()\n",
    "\n",
    "for i in range(41):\n",
    "    for j in range(i):\n",
    "        # i+1 and j+1 to skip the first row of classes\n",
    "        for k in range(5):\n",
    "            mask = (data[:,0] == k)\n",
    "            plt.plot(data[mask,i+1],data[mask,j+1],'+')\n",
    "        plt.xlabel(header[i])\n",
    "        plt.ylabel(header[j])\n",
    "        plt.savefig('imgs/'+header[i]+'-'+header[j]+'.png')\n",
    "        plt.close()\n",
    "        \n",
    "print 'done'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Class separability or class discriminatory power of the features.\n",
    "Calculate the Fisher's discriminant ratio for each feature and rank the features in descending order. The first feature in the list has the highest class separability of all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "standardize data...\n",
      "done\n",
      "calculate Fisher discriminant ratio (FDR)...\n",
      "done\n",
      "rank the features...\n",
      "   features sorted in order of how well they discriminate between different classes (first item is best):\n",
      "   ['ThetaPi_1' 'H2.H1_1' 'H1_1' 'Theta1Pi_1' 'H12_1' 'ThetaS_1' 'DAF_1'\n",
      " 'Theta1S_1' 'TajD_1' 'FuLiF_1' 'Theta1L_1' 'FuLiD_1' 'DXPEHH_12'\n",
      " 'FuLiF1_1' 'TajD1_1' 'DXPEHH_13\\n' 'FuLiD1_1' 'FayWuH_1' 'FST_1'\n",
      " 'XPEHH_12' 'SL1_1' 'H2_1' 'XPEHH_13' 'ThetaL_1' 'SL0_1' 'iHH0_1'\n",
      " 'Theta1H_1' 'DDAF_1' 'Theta1Xi_1' 'nSL_1' 'iHS_1' 'FayWuH1_1' 'DnSL_1'\n",
      " 'iHH1_1' 'ZengE1_1' 'MAF_1' 'ZengE_1' 'ZA_1' 'ThetaXi_1' 'DiHH_1'\n",
      " 'ThetaH_1']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "            header = np.array(header)\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "print 'standardize data...'\n",
    "# This is necessary because different features have different dynamic ranges. Standardization brings all features \n",
    "# to the same scale.\n",
    "\n",
    "mean = np.mean(data,axis=0)\n",
    "std = np.std(data,axis=0)\n",
    "\n",
    "data_standard = np.zeros(np.shape(data))\n",
    "data_mask = np.zeros(np.shape(data))\n",
    "# save the classes\n",
    "data_standard[:,0] = data[:,0]\n",
    "data_mask = np.isfinite(data)\n",
    "# standardize data\n",
    "for i in range(41):\n",
    "    a = data[:,i+1]\n",
    "    mask_a = data_mask[:,i+1]\n",
    "    mean = np.mean(a[mask_a]) \n",
    "    std = np.std(a[mask_a])\n",
    "    \n",
    "    data_standard[mask_a,i+1] = (a[mask_a] - mean)/std\n",
    "\n",
    "print 'done'\n",
    "\n",
    "print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "# FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "# sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "# The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "# are better at distinguishing classes.\n",
    "# For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "\n",
    "FDR = np.zeros(41)\n",
    "for i in range(41):\n",
    "    FDR_sum = 0e0\n",
    "    for j in range(5):\n",
    "        mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        for k in range(j):\n",
    "            mask_k = data_mask[:,i+1] & (data_standard[:,0] == k)\n",
    "            \n",
    "            mu_j = np.mean(data_standard[mask_j,i+1])\n",
    "            mu_k = np.mean(data_standard[mask_k,i+1])\n",
    "            sigma_j = np.var(data_standard[mask_j,i+1])\n",
    "            sigma_k = np.var(data_standard[mask_k,i+1])\n",
    "            \n",
    "            FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "    # check the values     \n",
    "    #print i,FDR_sum\n",
    "    #for j in range(5):\n",
    "    #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "    #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "    FDR[i] = FDR_sum\n",
    "print 'done'\n",
    "\n",
    "print 'rank the features...'\n",
    "indx_sorted = np.argsort(FDR)[::-1]\n",
    "print '   features sorted in order of how well they discriminate between different classes (first item is best):'\n",
    "print '  ',header[indx_sorted]\n",
    "\n",
    "# add 1 and insert 0 to the first place to keep the class in.\n",
    "indx_sorted = np.insert(indx_sorted+1,0,0)\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use an SVM to do classification\n",
    "separate the data into training, cross validation, and test data sets (60-20-20%).\n",
    "make a loop through successively less features:\n",
    "    - use all features to find the best values for C and gamma in the SVM\n",
    "    - fix the best C and gamma values and successively remove a feature that is least discriminative\n",
    "    - check what number of features give the best score in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the SVM and find the best C and gamma parameters...\n",
      "   round 0\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.530120481928\n",
      "      the cross validation score for C=10.0 and gamma=0.0001:\n",
      "          0.47619047619\n",
      "   round 1\n",
      "      best C value(s): [   10.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-04   1.00000000e-05]\n",
      "      max test score: 0.518072289157\n",
      "      the cross validation score for C=10.0 and gamma=0.0001:\n",
      "          0.488095238095\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.464285714286\n",
      "   round 2\n",
      "      best C value(s): [ 1.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.506024096386\n",
      "      the cross validation score for C=1.0 and gamma=0.0001:\n",
      "          0.464285714286\n",
      "   round 3\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.614457831325\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.52380952381\n",
      "   round 4\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.530120481928\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.416666666667\n",
      "   round 5\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.481927710843\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.488095238095\n",
      "   round 6\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.55421686747\n",
      "      the cross validation score for C=100.0 and gamma=1e-05:\n",
      "          0.547619047619\n",
      "   round 7\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.493975903614\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.452380952381\n",
      "   round 8\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.464285714286\n",
      "   round 9\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.44578313253\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.583333333333\n",
      "   round 10\n",
      "      best C value(s): [ 10.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.493975903614\n",
      "      the cross validation score for C=10.0 and gamma=0.0001:\n",
      "          0.607142857143\n",
      "   round 11\n",
      "      best C value(s): [    1.    10.   100.  1000.]\n",
      "      best gamma value(s): [  1.00000000e-03   1.00000000e-03   1.00000000e-05   1.00000000e-05]\n",
      "      max test score: 0.518072289157\n",
      "      the cross validation score for C=1.0 and gamma=0.001:\n",
      "          0.5\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.440476190476\n",
      "      the cross validation score for C=100.0 and gamma=1e-05:\n",
      "          0.52380952381\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.511904761905\n",
      "   round 12\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.542168674699\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.571428571429\n",
      "   round 13\n",
      "      best C value(s): [  10.  100.]\n",
      "      best gamma value(s): [ 0.0001  0.0001]\n",
      "      max test score: 0.518072289157\n",
      "      the cross validation score for C=10.0 and gamma=0.0001:\n",
      "          0.47619047619\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.404761904762\n",
      "   round 14\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.506024096386\n",
      "      the cross validation score for C=100.0 and gamma=1e-05:\n",
      "          0.511904761905\n",
      "   round 15\n",
      "      best C value(s): [   1.   10.   10.  100.]\n",
      "      best gamma value(s): [ 0.0001  0.0001  0.001   0.0001]\n",
      "      max test score: 0.542168674699\n",
      "      the cross validation score for C=1.0 and gamma=0.0001:\n",
      "          0.488095238095\n",
      "      the cross validation score for C=10.0 and gamma=0.0001:\n",
      "          0.511904761905\n",
      "      the cross validation score for C=10.0 and gamma=0.001:\n",
      "          0.47619047619\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.488095238095\n",
      "   round 16\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.578313253012\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.535714285714\n",
      "   round 17\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.590361445783\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.559523809524\n",
      "   round 18\n",
      "      best C value(s): [ 100.]\n",
      "      best gamma value(s): [ 0.0001]\n",
      "      max test score: 0.457831325301\n",
      "      the cross validation score for C=100.0 and gamma=0.0001:\n",
      "          0.547619047619\n",
      "   round 19\n",
      "      best C value(s): [ 1000.]\n",
      "      best gamma value(s): [  1.00000000e-05]\n",
      "      max test score: 0.469879518072\n",
      "      the cross validation score for C=1000.0 and gamma=1e-05:\n",
      "          0.52380952381\n",
      "done\n",
      "The best C and gamma values are 1000.0 and 0.0001 , respectively.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:26: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:27: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:29: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:30: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# run the previous cell first!\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# do everything 20 times to characterize the noise\n",
    "\n",
    "print 'Train the SVM and find the best C and gamma parameters...'\n",
    "C_collect = []\n",
    "gamma_collect = []\n",
    "for i in range(20):\n",
    "    print '   round',i\n",
    "\n",
    "    # I can only use those data points that do not contain NAs\n",
    "    mask_to_use = np.all(np.isfinite(data),axis=1)\n",
    "    data_to_use = data[mask_to_use,:]\n",
    "    # nr of data points left\n",
    "    n = np.shape(data_to_use)[0]\n",
    "    \n",
    "    # shuffle and divide up the data\n",
    "    indx = np.arange(n)\n",
    "    np.random.shuffle(indx)\n",
    "    \n",
    "    X_train = data_to_use[indx[:n*0.6],1:]\n",
    "    Y_train = data_to_use[indx[:n*0.6],0]\n",
    "    \n",
    "    X_test = data_to_use[indx[n*0.6:n*0.8],1:]\n",
    "    Y_test = data_to_use[indx[n*0.6:n*0.8],0]\n",
    "    \n",
    "    X_CV = data_to_use[indx[n*0.8:],1:]\n",
    "    Y_CV = data_to_use[indx[n*0.8:],0]\n",
    "    \n",
    "    # parameter ranges for the SVM\n",
    "    C = 10e0**(np.linspace(-8e0,3e0,12))\n",
    "    gamma = 10e0**(np.linspace(-5e0,6e0,12))\n",
    "    \n",
    "    # arrays to store scores and best parameters\n",
    "    train_score = np.zeros([len(C),len(gamma)])\n",
    "    test_score = np.zeros([len(C),len(gamma)])\n",
    "    C_array = np.zeros([len(C),len(gamma)])\n",
    "    gamma_array = np.zeros([len(C),len(gamma)])\n",
    "    # loop through the C and gamma arrays\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(gamma)):\n",
    "            SVM = SVC(kernel='rbf', C=C[i], gamma=gamma[j]).fit(X_train, Y_train)\n",
    "            train_score[i,j] = SVM.score(X_train,Y_train)\n",
    "            test_score[i,j] = SVM.score(X_test,Y_test)\n",
    "            C_array[i,j] = C[i]\n",
    "            gamma_array[i,j] = gamma[j]\n",
    "\n",
    "    # find the C and gamma parameters that give max score.\n",
    "    # if there are multiple parameter configuration giving max score, the first one of these is used below \n",
    "    best_params = np.where(test_score == np.max(test_score))\n",
    "    print '      best C value(s):',C[best_params[0]]\n",
    "    print '      best gamma value(s):',gamma[best_params[1]]\n",
    "    print '      max test score:',np.max(test_score)\n",
    "    # calculate the cross validation score\n",
    "    for j in range(len(best_params[0])):\n",
    "        print '      the cross validation score for C='+str(C[best_params[0][j]])+' and gamma='+str(gamma[best_params[1][j]])+':'\n",
    "        SVM = SVC(kernel='rbf', C=C[best_params[0][j]], gamma=gamma[best_params[1][j]]).fit(X_train, Y_train)\n",
    "        CV_score = SVM.score(X_CV,Y_CV)\n",
    "        print '         ',CV_score\n",
    "    \n",
    "    C_collect.append(C[best_params[0]])\n",
    "    gamma_collect.append(gamma[best_params[1]])\n",
    "\n",
    "print 'done'\n",
    "\n",
    "# flatten the *_collect lists and find the most frequently occuring elements\n",
    "C_flat = [item for sublist in C_collect for item in sublist]\n",
    "gamma_flat = [item for sublist in gamma_collect for item in sublist]\n",
    "\n",
    "C = max(set(C_flat), key=C_flat.count)\n",
    "gamma = max(set(gamma_flat), key=gamma_flat.count)\n",
    "\n",
    "print 'The best C and gamma values are',C,'and',gamma,', respectively.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove one feature at a time (the least discriminative one) and calculate the classification score...\n",
      "    0\n",
      "    10\n",
      "    20\n",
      "    30\n",
      "    40\n",
      "    50\n",
      "    60\n",
      "    70\n",
      "    80\n",
      "    90\n",
      "    100\n",
      "    110\n",
      "    120\n",
      "    130\n",
      "    140\n",
      "    150\n",
      "    160\n",
      "    170\n",
      "    180\n",
      "    190\n",
      "    200\n",
      "    210\n",
      "    220\n",
      "    230\n",
      "    240\n",
      "    250\n",
      "    260\n",
      "    270\n",
      "    280\n",
      "    290\n",
      "    300\n",
      "    310\n",
      "    320\n",
      "    330\n",
      "    340\n",
      "    350\n",
      "    360\n",
      "    370\n",
      "    380\n",
      "    390\n",
      "    400\n",
      "    410\n",
      "    420\n",
      "    430\n",
      "    440\n",
      "    450\n",
      "    460\n",
      "    470\n",
      "    480\n",
      "    490\n",
      "    500\n",
      "    510\n",
      "    520\n",
      "    530\n",
      "    540\n",
      "    550\n",
      "    560\n",
      "    570\n",
      "    580\n",
      "    590\n",
      "    600\n",
      "    610\n",
      "    620\n",
      "    630\n",
      "    640\n",
      "    650\n",
      "    660\n",
      "    670\n",
      "    680\n",
      "    690\n",
      "    700\n",
      "    710\n",
      "    720\n",
      "    730\n",
      "    740\n",
      "    750\n",
      "    760\n",
      "    770\n",
      "    780\n",
      "    790\n",
      "    800\n",
      "    810\n",
      "    820\n",
      "    830\n",
      "    840\n",
      "    850\n",
      "    860\n",
      "    870\n",
      "    880\n",
      "    890\n",
      "    900\n",
      "    910\n",
      "    920\n",
      "    930\n",
      "    940\n",
      "    950\n",
      "    960\n",
      "    970\n",
      "    980\n",
      "    990\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:24: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:26: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:27: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "print 'Remove one feature at a time (the least discriminative one) and calculate the classification score...'\n",
    "\n",
    "n_sim = 1000\n",
    "train_score = np.zeros([n_sim,40])\n",
    "test_score = np.zeros([n_sim,40])\n",
    "\n",
    "for i in range(n_sim):\n",
    "    if i%10 == 0:\n",
    "        print '   ',i\n",
    "    for j in range(40):        \n",
    "        # I can only use those data points that do not contain NAs\n",
    "        mask_to_use = np.all(np.isfinite(data[:,indx_sorted[:-(j+1)]]),axis=1)\n",
    "        data_to_use = data[:,indx_sorted[:-(j+1)]]\n",
    "        data_to_use = data_to_use[mask_to_use,:]\n",
    "                        \n",
    "        # nr of data points left\n",
    "        n = np.shape(data_to_use)[0]\n",
    "                \n",
    "        # shuffle and divide up the data\n",
    "        indx = np.arange(n)\n",
    "        np.random.shuffle(indx)\n",
    "\n",
    "        X_train = data_to_use[indx[:n*0.75],1:]\n",
    "        Y_train = data_to_use[indx[:n*0.75],0]\n",
    "\n",
    "        X_test = data_to_use[indx[n*0.75:],1:]\n",
    "        Y_test = data_to_use[indx[n*0.75:],0]\n",
    "\n",
    "        SVM = SVC(kernel='rbf', C=C, gamma=gamma).fit(X_train, Y_train)\n",
    "        train_score[i,j] = SVM.score(X_train,Y_train)\n",
    "        test_score[i,j] = SVM.score(X_test,Y_test)\n",
    "                \n",
    "\n",
    "# make a plot            \n",
    "plt.close()\n",
    "plt.errorbar(np.arange(40)+1,np.average(test_score,axis=0)*100e0,yerr=np.std(test_score,axis=0)*100,fmt='o')\n",
    "plt.xlabel('nr. of features used')\n",
    "plt.ylabel('test score [%]')\n",
    "plt.savefig('classification_'+str(n_sim)+'.png')\n",
    "plt.close()\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
