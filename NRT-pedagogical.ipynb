{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation coefficient matrix and the cloud of points for every pair of variables. This helps to familiarize myself with the data, it allows me to check how strongly correlated some variables are and what type of correlation is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "prepare the plots...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "print 'prepare the plots...'\n",
    "\n",
    "# correlation coefficient matrix, masking out nans (didn't find a way to do this without for loops)\n",
    "corr_coef = np.zeros([41,41])\n",
    "for i in range(41):\n",
    "    for j in range(41):\n",
    "        a = data[:,i+1]\n",
    "        b = data[:,j+1]\n",
    "        mask_a = np.isfinite(a)\n",
    "        mask_b = np.isfinite(b)\n",
    "        mask = mask_a & mask_b\n",
    "        corr_coef[i,j] = np.corrcoef([a[mask],b[mask]])[0,1]\n",
    "\n",
    "plt.title('white - no corr., red - positive corr., blue - negative corr.')\n",
    "plt.imshow(corr_coef,cmap=\"bwr\",interpolation=\"nearest\")\n",
    "plt.savefig('corr_coef.png')\n",
    "plt.close()\n",
    "\n",
    "for i in range(41):\n",
    "    for j in range(i):\n",
    "        # i+1 and j+1 to skip the first row of classes\n",
    "        for k in range(5):\n",
    "            mask = (data[:,0] == k)\n",
    "            plt.plot(data[mask,i+1],data[mask,j+1],'+')\n",
    "        plt.xlabel(header[i])\n",
    "        plt.ylabel(header[j])\n",
    "        plt.savefig('imgs/'+header[i]+'-'+header[j]+'.png')\n",
    "        plt.close()\n",
    "        \n",
    "print 'done'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Class separability or class discriminatory power of the features.\n",
    "Calculate the Fisher's discriminant ratio for each feature and rank the features in descending order. The first feature in the list has the highest class separability of all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data files...\n",
      "done\n",
      "standardize data...\n",
      "done\n",
      "calculate Fisher discriminant ratio (FDR)...\n",
      "done\n",
      "rank the features...\n",
      "   features sorted in order of how well they discriminate between different classes (first item is best):\n",
      "   ['ThetaPi_1' 'H2.H1_1' 'H1_1' 'Theta1Pi_1' 'H12_1' 'ThetaS_1' 'DAF_1'\n",
      " 'Theta1S_1' 'TajD_1' 'FuLiF_1' 'Theta1L_1' 'FuLiD_1' 'DXPEHH_12'\n",
      " 'FuLiF1_1' 'TajD1_1' 'DXPEHH_13\\n' 'FuLiD1_1' 'FayWuH_1' 'FST_1'\n",
      " 'XPEHH_12' 'SL1_1' 'H2_1' 'XPEHH_13' 'ThetaL_1' 'SL0_1' 'iHH0_1'\n",
      " 'Theta1H_1' 'DDAF_1' 'Theta1Xi_1' 'nSL_1' 'iHS_1' 'FayWuH1_1' 'DnSL_1'\n",
      " 'iHH1_1' 'ZengE1_1' 'MAF_1' 'ZengE_1' 'ZA_1' 'ThetaXi_1' 'DiHH_1'\n",
      " 'ThetaH_1']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = ['data-hl.txt','data-hs.txt','data-ne.txt','data-sl.txt','data-ss.txt']\n",
    "data = np.zeros([5*300,42])\n",
    "\n",
    "print 'read data files...'\n",
    "i = 0\n",
    "for file in files:\n",
    "    j = 0\n",
    "    for line in open(file,'r'):\n",
    "        if line.split('\\t')[0] != 'sclass':\n",
    "            values = np.array(line.split('\\t'))\n",
    "            values[values == 'NA'] = 'nan'\n",
    "            values[values == 'NA\\n'] = 'nan'\n",
    "            # save the class: points in data-hl.txt have class 0, points in data-hs.txt are in class 1, etc.\n",
    "            data[i*300+j,0] = i\n",
    "            data[i*300+j,1:] = values[1:].astype(float)\n",
    "            j = j + 1\n",
    "        else:\n",
    "            header = line.split('\\t')[1:]\n",
    "            header = np.array(header)\n",
    "    i = i + 1 \n",
    "print 'done'    \n",
    "\n",
    "print 'standardize data...'\n",
    "# This is necessary because different features have different dynamic ranges. Standardization brings all features \n",
    "# to the same scale.\n",
    "\n",
    "mean = np.mean(data,axis=0)\n",
    "std = np.std(data,axis=0)\n",
    "\n",
    "data_standard = np.zeros(np.shape(data))\n",
    "data_mask = np.zeros(np.shape(data))\n",
    "# save the classes\n",
    "data_standard[:,0] = data[:,0]\n",
    "data_mask = np.isfinite(data)\n",
    "# standardize data\n",
    "for i in range(41):\n",
    "    a = data[:,i+1]\n",
    "    mask_a = data_mask[:,i+1]\n",
    "    mean = np.mean(a[mask_a]) \n",
    "    std = np.std(a[mask_a])\n",
    "    \n",
    "    data_standard[mask_a,i+1] = (a[mask_a] - mean)/std\n",
    "\n",
    "print 'done'\n",
    "\n",
    "print 'calculate Fisher discriminant ratio (FDR)...'\n",
    "# FDR = sum sum (mu_i - mu_j)^2 / (sigma_i^2 + sigma_j^2), where i and j are different classes, mu is the mean, \n",
    "# sigma is the variance. Sum goes over all i-j pairs excluding i=j. \n",
    "# The idea is that features with large differences in the class-specific means and small variances in each class\n",
    "# are better at distinguishing classes.\n",
    "# For more details on FDR see e.g., Lin et al., J. Chem. Inf. Comput. Sci. 2004, 44, 76-87\n",
    "\n",
    "FDR = np.zeros(41)\n",
    "for i in range(41):\n",
    "    FDR_sum = 0e0\n",
    "    for j in range(5):\n",
    "        mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "        for k in range(j):\n",
    "            mask_k = data_mask[:,i+1] & (data_standard[:,0] == k)\n",
    "            \n",
    "            mu_j = np.mean(data_standard[mask_j,i+1])\n",
    "            mu_k = np.mean(data_standard[mask_k,i+1])\n",
    "            sigma_j = np.var(data_standard[mask_j,i+1])\n",
    "            sigma_k = np.var(data_standard[mask_k,i+1])\n",
    "            \n",
    "            FDR_sum = FDR_sum + (mu_j-mu_k)**2e0 / (sigma_j**2e0 + sigma_k**2e0)\n",
    "    # check the values     \n",
    "    #print i,FDR_sum\n",
    "    #for j in range(5):\n",
    "    #    mask_j = data_mask[:,i+1] & (data_standard[:,0] == j)\n",
    "    #    print '   ',np.mean(data_standard[mask_j,i+1])/np.var(data_standard[mask_j,i+1])\n",
    "    FDR[i] = FDR_sum\n",
    "print 'done'\n",
    "\n",
    "print 'rank the features...'\n",
    "indx_sorted[::-1] = np.argsort(FDR)\n",
    "print '   features sorted in order of how well they discriminate between different classes (first item is best):'\n",
    "print '  ',header[indx_sorted]\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use an SVM to do classification\n",
    "separate the data into training, cross validation, and test data sets (60-20-20%).\n",
    "make a loop through successively less features:\n",
    "    - use all features to find the best values for C and gamma in the SVM\n",
    "    - fix the best C and gamma values and successively remove a feature that is least discriminative\n",
    "    - check what number of features give the best score in classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
